# Here are collections about other CV topics

**Contents Hyperlinks**

* [‚≠ê3D Object Detection](#3d-object-detection)
* [‚≠ê3D Reconstruction](#3d-reconstruction)
* [‚≠ê6D Object Pose Estimation](#6d-object-pose-estimation)
* [‚≠êAerial Autonomous Navigation](#aerial-autonomous-navigation)
* [‚≠êCamera Pose Estimation (SLAM)](#camera-pose-estimation-slam)
* [‚≠êDeep Neural Networks](#deep-neural-networks)
* [‚≠êGenerative Adversarial Network](#generative-adversarial-network)
* [‚≠êHuman Object Interaction Detection](#human-object-interaction-detection)
* [‚≠êImage Mosaic](#image-mosaic)
* [‚≠êImage Restoration](#image-restoration)
* [‚≠êLane Detection](#lane-detection)
* [‚≠êPedestrian Localization](#pedestrian-localization)
* [‚≠êPerson ReID](#person-reid)
* [‚≠êScene Text Detection](#scene-text-detection)
* [‚≠êSemantic Segmentation](#semantic-segmentation)
* [‚≠êSound Source Localization](#sound-source-localization)
* [‚≠êTraffic Violation Detection](#traffic-violation-detection)


**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠ê3D Object Detection

### Materials


### Papers

* ‚ù§ **3D-BoundingBox(CVPR2017)** 3D Bounding Box Estimation Using Deep Learning and Geometry [[paper link](https://arxiv.org/abs/1612.00496)][[codes|official PyTorch](https://github.com/skhadem/3D-BoundingBox)]

* ‚ù§ **SMOKE(CVPRW2020)** SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation [[paper link](https://openaccess.thecvf.com/content_CVPRW_2020/html/w60/Liu_SMOKE_Single-Stage_Monocular_3D_Object_Detection_via_Keypoint_Estimation_CVPRW_2020_paper.html)][[codes|official PyTorch](https://github.com/lzccccc/SMOKE)]

* **MonoPair(CVPR2020)** MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships [[paper link](https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_MonoPair_Monocular_3D_Object_Detection_Using_Pairwise_Spatial_Relationships_CVPR_2020_paper.html)][[codes|]()]

* **RTM3D(ECCV2020)** RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving [[paper link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480647.pdf)][[codes|]()]

* ‚ù§ **FADNet(TIV2022)** Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation [[paper link](https://arxiv.org/abs/2011.14589)][[codes|official](https://github.com/gtzly/FADNet)]

**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠ê3D Reconstruction

### Materials


### Papers

* **PSVH-3d-reconstruction(AAAI2019)** Deep Single-View 3D Object Reconstruction with Visual Hull Embedding [[paper link](https://ojs.aaai.org/index.php/AAAI/article/view/4922)][[codes|official TensorFlow](https://github.com/HanqingWangAI/PSVH-3d-reconstruction)]

* **‚ù§ ROMP(ICCV2021)** Monocular, One-stage, Regression of Multiple 3D People [[paper link](https://openaccess.thecvf.com/content/ICCV2021/html/Sun_Monocular_One-Stage_Regression_of_Multiple_3D_People_ICCV_2021_paper.html)][[codes|official](https://github.com/Arthur151/ROMP)]

* **(arxiv2022)** A Real World Dataset for Multi-view 3D Reconstruction [[paper link](https://arxiv.org/abs/2203.11397)]

* **SDF(CVPR2022)** Neural 3D Scene Reconstruction with the Manhattan-world Assumption [[paper link](https://arxiv.org/abs/2205.02836)][[project link](https://zju3dv.github.io/manhattan_sdf/)][[codes|official PyTorch](https://github.com/zju3dv/manhattan_sdf)]

* **‚ù§ BEV(CVPR2022)** Putting People in their Place: Monocular Regression of 3D People in Depth [[paper link](https://openaccess.thecvf.com/content/CVPR2022/html/Sun_Putting_People_in_Their_Place_Monocular_Regression_of_3D_People_CVPR_2022_paper.html)][[project link](https://arthur151.github.io/BEV/BEV.html)][[codes|official](https://github.com/Arthur151/ROMP)][[Relative Human dataset](https://github.com/Arthur151/Relative_Human)]

* **Survey(arxiv2022)** Recovering 3D Human Mesh from Monocular Images: A Survey [[paper link](https://arxiv.org/abs/2203.01923)][[codes|official](https://github.com/tinatiansjz/hmr-survey)]


**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠ê6D Object Pose Estimation

### Materials

* [(zhihu) ËØùÈ¢òÔºö6DOFÂßøÊÄÅ‰º∞ËÆ°](https://www.zhihu.com/collection/274088096)
* [(website) A Comprehensive List of 3D Sensors Commonly Leveraged in ROS Development](https://rosindustrial.org/3d-camera-survey)
* [(CSDN blogs) „ÄêÊ∑±Â∫¶Áõ∏Êú∫Á≥ªÂàóÂÖ≠„ÄëÊ∑±Â∫¶Áõ∏Êú∫Âì™ÂÆ∂Âº∫ÔºüÈôÑËØ¶ÁªÜÂèÇÊï∞ÂØπÊØîÊ∏ÖÂçï](https://blog.csdn.net/electech6/article/details/78907463)
* [(website) List of RGBD datasets](http://www.michaelfirman.co.uk/RGBDdatasets/) (written by a domain expert [Michael Firman](http://www.michaelfirman.co.uk/index.html))
* [(CSDN blogs) 6DÂßøÊÄÅ‰º∞ËÆ°ÁÆóÊ≥ïÊ±áÊÄªÔºà‰∏äÔºâ](https://blog.csdn.net/qq_29462849/article/details/103740960)
* [(CSDN blogs) 6DÂßøÊÄÅ‰º∞ËÆ°ÁÆóÊ≥ïÊ±áÊÄªÔºà‰∏ãÔºâ](https://blog.csdn.net/qq_29462849/article/details/103741059)
* [(zhihu) VRËÆæÂ§áÂ∏∏ËØ¥ÁöÑ3DOFÂíå6DOFÂà∞Â∫ïÊòØ‰ªÄ‰πàÔºü](https://zhuanlan.zhihu.com/p/114650000)
* [(github) Awesome work on object 6 DoF pose estimation](https://github.com/ZhongqunZHANG/awesome-6d-object)
* [(github) MediaPipe: About Cross-platform, customizable ML solutions for live and streaming media](https://github.com/google/mediapipe)
* [(tools) ARCore: The AR session captures camera poses, point-clouds, and surface planes](http://developers.google.com/ar/)
* [(tools) ARKit: The AR session captures camera poses, point-clouds, and surface planes](https://developer.apple.com/augmented-reality/)
* [(website) paperswithcode: 6D Pose Estimation using RGB](https://paperswithcode.com/task/6d-pose-estimation)
* [(toolkit) 3D Annotation Of Arbitrary Objects In The Wild (inputs are RGB-D)](https://docs.strayrobots.io/toolkit/index.html) [[paper link](https://arxiv.org/abs/2109.07165)]
* [(algorithm) EPnP: Efficient Perspective-n-Point Camera Pose Estimation](https://www.epfl.ch/labs/cvlab/software/multi-view-stereo/epnp/)
* [(blogs) 3D Object DetectionÂíå6D Pose EstimationÊúâ‰ªÄ‰πàÂºÇÂêåÔºü](https://www.bilibili.com/read/cv5287260)

### Datasets

* [3DObject (ICCV2007)](http://vision.stanford.edu/resources_links.html): 3D generic object categorization, localization and pose estimation [***It provides discretized viewpoint annotations for 10 everyday object categories***]
* [‚ù§ LineMOD (ACCV2012)](https://campar.in.tum.de/Main/StefanHinterstoisser): Model Based Training, Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes [***The most commonly used dataset for object pose estimation***]
* [Pascal3D+ (WACV2014)](https://cvgl.stanford.edu/projects/pascal3d): Beyond PASCAL - A Benchmark for 3D Object Detection in the Wild [***It adds 3D pose annotations to the Pascal VOC and a few images from the ImageNet dataset***]
* [‚ù§ ShapeNet (arxiv2015)](https://shapenet.org/): ShapeNet - An Information-Rich 3D Model Repository [***It includes synthetic CAD models for many objects and has been widely used***]
* [IC-BIN dataset (CVPR2016)](https://bop.felk.cvut.cz/leaderboards/bop19_ic-bin/): Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd [***It adds a few more categories based on LineMOD***]
* [Rutgers APC (ICRA2016)](https://robotics.cs.rutgers.edu/pracsys/rutgers-apc-rgb-d-dataset/): A Dataset for Improved RGBD-based Object Detection and Pose Estimation for Warehouse Pick-and-Place [***It contains 14 textured objects from the Amazon picking challenge***]
* [ObjectNet3D (ECCV2016)](https://cvgl.stanford.edu/projects/objectnet3d/): ObjectNet3D - A Large Scale Database for 3D Object Recognition [***It contains 3D object poses from images***]
* [‚ù§ T-LESS (WACV2017)](https://cmp.felk.cvut.cz/t-less/): T-LESS - An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects [***It features industrialized objects that lack texture or color***]
* [YCB (IJRR2017)](http://www.ycbbenchmarks.org/): Yale-CMU-Berkeley dataset for robotic manipulation research [***It contains videos of objects and their poses in a controlled environment***]
* [ScanNet(CVPR2017)](http://www.scan-net.org/): ScanNet - Richly-Annotated 3D Reconstructions of Indoor Scenes [***A large scale video dataset of indoor scenes with semantic annotations***]
* [‚ù§ BOP Challenge (ECCV2018)](https://bop.felk.cvut.cz/home/): BOP - Benchmark for 6D Object Pose Estimation [***It consists of a set of benchmark for 3D object detection and combines many of these smaller datasets into a larger one***]
* [Pix3D (CVPR2018)](http://pix3d.csail.mit.edu/): Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling [***It contains pixel-level 2D-3D pose alignment***]
* [Scan2CAD (CVPR2019)](scan2cad.org): Scan2CAD - Learning CAD Model Alignment in RGB-D Scans [***It annotates the original scans in ScanNet with ShapeNetCore models to label each object‚Äôs pose***]
* [RIO (ICCV2019)](https://waldjohannau.github.io/RIO/): RIO - 3D Object Instance Re-Localization in Changing Indoor Environments [***Another dataset that contains indoor scans annotated with an object‚Äôs 3D pose***]
* [NOCS (CVPR2019 Oral)](https://geometry.stanford.edu/projects/NOCS_CVPR2019/): Normalized Object Coordinate Space (NOCS) - a shared canonical representation for all possible object instances within a category [***It is a fully annotated real-world RGB-D dataset with large environment and instance variation***]
* [‚ù§ Objectron (CVPR2021)](https://github.com/google-research-datasets/Objectron): A Large Scale Dataset of Object-Centric Videos in the Wild With Pose Annotations
* [‚ù§ PhoCaL (CVPR2022)](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_PhoCaL_A_Multi-Modal_Dataset_for_Category-Level_Object_Pose_Estimation_With_CVPR_2022_paper.html): PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation With Photometrically Challenging Objects [***A novel robot-supported multi-modal (RGB, depth, polarisation) benchmark with challenging scenes supporting RGB-D and monocular RGB methods***]


### Papers

* **Pascal3D+(WACV2014)** Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild [[paper link](http://roozbehm.info/papers/Xiang14wacv.pdf)][[project link](https://cvgl.stanford.edu/projects/pascal3d)]

* **(ICCVW2017)** 3D Pose Regression Using Convolutional Neural Networks [[paper link](https://openaccess.thecvf.com/content_ICCV_2017_workshops/w31/html/Mahendran_3D_Pose_Regression_ICCV_2017_paper.html)]

* **PoseCNN(RSS2018)** PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes [[paper link](https://yuxng.github.io/xiang_rss18.pdf)][[project link](https://rse-lab.cs.washington.edu/projects/posecnn/)]

* **(ECCV2018)** Occlusion Resistant Object Rotation Regression from Point Cloud Segments [[paper link](https://openaccess.thecvf.com/content_eccv_2018_workshops/w6/html/Gao_Occlusion_Resistant_Object_Rotation_Regression_from_Point_Cloud_Segments_ECCVW_2018_paper.html)]

* **DeepIM(ECCV2018)** DeepIM: Deep Iterative Matching for 6D Pose Estimation [[paper link](https://openaccess.thecvf.com/content_ECCV_2018/html/Yi_Li_DeepIM_Deep_Iterative_ECCV_2018_paper.html)][[project link](https://rse-lab.cs.washington.edu/projects/deepim/)]

* ‚ù§**YOLO-6D(CVPR2018)** Real-Time Seamless Single Shot 6D Object Pose Prediction [[paper link](https://openaccess.thecvf.com/content_cvpr_2018/html/Tekin_Real-Time_Seamless_Single_CVPR_2018_paper.html)][[codes|official PyTorch](https://github.com/microsoft/singleshotpose)][[codes|unofficial TensorFlow](https://github.com/Mmmofan/YOLO_6D)][`YOLOv2`]

* ‚ù§**YOLO-Seg(CVPR2019)** Segmentation-Driven 6D Object Pose Estimation [[paper link](https://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Segmentation-Driven_6D_Object_Pose_Estimation_CVPR_2019_paper.html)][[codes|official](https://github.com/cvlab-epfl/segmentation-driven-pose)][`YOLOv2`]

* ‚ù§**PVNet(CVPR2019 Oral)** PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation [[paper link](https://openaccess.thecvf.com/content_CVPR_2019/html/Peng_PVNet_Pixel-Wise_Voting_Network_for_6DoF_Pose_Estimation_CVPR_2019_paper.html)][[codes|official](https://zju3dv.github.io/pvnet/)]

* **NOCS(CVPR2019 Oral)** Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation [[paper link](https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2019_paper.html)][[project link](https://geometry.stanford.edu/projects/NOCS_CVPR2019/)][[codes & datasets|official keras and tensorflow](https://github.com/hughw19/NOCS_CVPR2019)]

* **DPOD(ICCV2019)** DPOD: 6D Pose Object Detector and Refiner [[paper link](https://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.html)][[codes|PyTorch](https://github.com/zakharos/DPOD)]

* **CDPN(ICCV2019)** CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation [[paper link](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.html)][[codes|official PyTorch](https://github.com/LZGMatrix/CDPN_ICCV2019_ZhigangLi)][`YOLOv3`]

* **HybridPose(CVPR2020)** HybridPose: 6D Object Pose Estimation Under Hybrid Representations [[paper link](https://openaccess.thecvf.com/content_CVPR_2020/html/Song_HybridPose_6D_Object_Pose_Estimation_Under_Hybrid_Representations_CVPR_2020_paper.html)]

* **single-stage-pose(CVPR2020)** Single-Stage 6D Object Pose Estimation [[paper link](https://openaccess.thecvf.com/content_CVPR_2020/html/Hu_Single-Stage_6D_Object_Pose_Estimation_CVPR_2020_paper.html)][[codes|official PyTorch](https://github.com/cvlab-epfl/single-stage-pose)]

* **CosyPose(ECCV2020)** CosyPose: Consistent Multi-view Multi-object 6D Pose Estimation [[paper link](https://hal.inria.fr/hal-02950800/)][[project link](https://www.di.ens.fr/willow/research/cosypose)][[codes|official PyTorch](https://github.com/ylabbe/cosypose)]

* **MobilePose(arxiv2020)** MobilePose: Real-Time Pose Estimation for Unseen Objects with Weak Shape Supervision [[paper link](https://arxiv.org/abs/2003.03522)]

* **SGPA(ICCV2021)** SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation [[paper link](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.html)][[codes|PyTorch](https://github.com/leo94-hk/SGPA)]

* ‚ù§**Objectron(CVPR2021)(Training Codes ‚ï≥)(Annotation Tool ‚ï≥)]** Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild With Pose Annotations [[paper link](https://openaccess.thecvf.com/content/CVPR2021/html/Ahmadyan_Objectron_A_Large_Scale_Dataset_of_Object-Centric_Videos_in_the_CVPR_2021_paper.html)][[codes|PyTorch+TensorFlow](https://github.com/google-research-datasets/Objectron)][[official blog 1: MediaPipe](https://mediapipe.dev/)][[official blog 2: MediaPipe Objectron](https://google.github.io/mediapipe/solutions/objectron)]

* **SAR-Net(CVPR2022)** SAR-Net: Shape Alignment and Recovery Network for Category-level 6D Object Pose and Size Estimation [[paper link](https://arxiv.org/abs/2106.14193)][[project link](https://hetolin.github.io/SAR-Net/)][[codes|official](https://github.com/hetolin/SAR-Net)]

* **OVE6D-pose(CVPR2022)** OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation [[paper link](https://arxiv.org/pdf/2203.01072.pdf)][[project link](https://dingdingcai.github.io/ove6d-pose/)][[codes|official](https://github.com/dingdingcai/OVE6D-pose)]

* **OnePose(CVPR2022)** OnePose: One-Shot Object Pose Estimation without CAD Models [[paper link](https://arxiv.org/pdf/2205.12257.pdf)][[project link](https://zju3dv.github.io/onepose/)][[codes|official](https://github.com/zju3dv/OnePose)][`ZJU + Objectron`]

* **Gen6D(ECCV2022)** Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images [[paper link](https://arxiv.org/abs/2204.10776)][[project link](https://liuyuan-pal.github.io/Gen6D/)][[codes|on the way]()]

* ‚ù§**CenterSnap(ICRA2022)** CenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation [[paper link](https://arxiv.org/abs/2203.01929)][[project link](https://zubair-irshad.github.io/projects/CenterSnap.html)][[codes|official PyTorch](https://github.com/zubair-irshad/CenterSnap)]

* ‚ù§**CenterPose(ICRA2022)(Training with CenterNet and Objectron)** Single-stage Keypoint-based Category-level Object Pose Estimation from an RGB Image [[paper link](https://arxiv.org/abs/2109.06161)][[project link](https://sites.google.com/view/centerpose)][[author homepage](https://yunzhi.netlify.app/)][[codes|official PyTorch](https://github.com/NVlabs/CenterPose)][`Nvidia + Objectron + one-stage + end2end`][based on `FADNet` (https://github.com/gtzly/FADNet) and `CenterNet` (https://github.com/xingyizhou/CenterNet)]


**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êAerial Autonomous Navigation

### Materials

* [(github) CMU: Leveraging system development and robot deployment for aerial autonomous navigation.](https://github.com/caochao39/aerial_navigation_development_environment) [[demo video](https://www.bilibili.com/video/BV1tZ4y187HR)]

### Papers


**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êCamera Pose Estimation (SLAM)

### Materials

* [(cnblogs) ËßÜËßâSLAM(Simultaneous Localization and Mapping)Êº´Ë∞à](https://blog.csdn.net/weixin_41537599/article/details/110819969)

### Papers

* **ORB-SLAM(TRO2015)** ORB-SLAM: a Versatile and Accurate Monocular SLAM System [[paper link](https://arxiv.org/abs/1502.00956)][[project link](http://webdiis.unizar.es/~raulmur/orbslam/)][[codes|official ROS](https://github.com/raulmur/ORB_SLAM)]

* **ORB-SLAM2(TRO2017)** ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras [[paper link](https://arxiv.org/abs/1610.06475)][[project link](http://webdiis.unizar.es/~raulmur/orbslam/)][[codes|official ROS](https://github.com/raulmur/ORB_SLAM2)]

* **GeoNet(CVPR2018)** GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose [[paper link](https://openaccess.thecvf.com/content_cvpr_2018/html/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.html)][[codes|official Tensorflow](https://github.com/yzcjtr/GeoNet)]

* **DeepVO(ICRA2017)** DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks [[paper link](https://arxiv.org/pdf/1709.08429.pdf)][[project link](http://senwang.gitlab.io/DeepVO/)][[codes|unofficial PyTorch 1](https://github.com/ChiWeiHsiao/DeepVO-pytorch)][[codes|unofficial PyTorch 2](https://github.com/krrish94/DeepVO)]

* **BiLevelOpt(3DV2020)** Joint Unsupervised Learning of Optical Flow and Egomotion with Bi-Level Optimization [[paper link](https://arxiv.org/abs/2002.11826)]

* **TartanVO(CoRL2021)** TartanVO: A Generalizable Learning-based VO [[paper link](https://proceedings.mlr.press/v155/wang21h.html)][[codes|official PyTorch](https://github.com/castacks/tartanvo)]

* **‚ù§ DiffPoseNet(CVPR2022)** DiffPoseNet: Direct Differentiable Camera Pose Estimation [[paper link](https://arxiv.org/abs/2203.11174)][[first author](https://analogicalnexus.github.io/)][[project link](https://nitinjsanket.github.io/research.html)][[codes|official PyTorch]()]


**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êGenerative Adversarial Network

### Materials

* [(blog) Test and Train CycleGAN](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb#scrollTo=OzSKIPUByfiN)
* [(CSDNblog) CycleGANËÆ∫ÊñáÁöÑÈòÖËØª‰∏éÁøªËØëÔºåÊó†ÁõëÁù£È£éÊ†ºËøÅÁßª](https://zhuanlan.zhihu.com/p/45394148)
* [(CSDNblog) ÁîüÊàêÂØπÊäóÁΩëÁªú(Âõõ)CycleGANËÆ≤Ëß£](https://blog.csdn.net/qq_40520596/article/details/104714762)
* [(blog) What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)

### Papers

* ‚ù§ **CycleGAN(ICCV2017)** Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [[arxiv link](https://arxiv.org/pdf/1703.10593.pdf)][[project link](https://junyanz.github.io/CycleGAN/)][[Codes|PyTorch(official)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)]

* ‚ù§ **CUT(ECCV2020)** Contrastive Learning for Unpaired Image-to-Image Translation [[arxiv link](https://arxiv.org/abs/2007.15651)][[project link](http://taesung.me/ContrastiveUnpairedTranslation/)][[Codes|PyTorch(official)](https://github.com/taesungp/contrastive-unpaired-translation)]

* ‚ù§ **GET3D(NIPS2022)** GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images [[paper link](https://nv-tlabs.github.io/GET3D/assets/paper.pdf)][[project link](https://nv-tlabs.github.io/GET3D/)][[codes|official PyTorch](https://github.com/nv-tlabs/GET3D)][`NVIDIA`]

* ‚ù§ **SCAM(ECCV2022)** SCAM! Transferring humans between images with Semantic Cross Attention Modulation [[paper link](https://arxiv.org/abs/2210.04883)][[project link](https://imagine.enpc.fr/~dufourn/publications/scam.html)][[codes|official PyTorch](https://github.com/nicolas-dufour/SCAM)]

* **SDEdit(ICLR2022)** SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations [[paper link](https://arxiv.org/abs/2108.01073)][[project link](https://sde-image-editing.github.io/)][`Partial StyleGAN`]

**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êHuman Object Interaction Detection

### Materials

* [(DEtection TRansformer(DETR) ECCV2020 BestPaper) End-to-End Object Detection with Transformers](https://link.springer.com/chapter/10.1007/978-3-030-58452-8_13) [[codes|official](https://github.com/facebookresearch/detr)]
* [(DAB-DETR ICLR2022) DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://arxiv.org/abs/2201.12329) [[codes|official](https://github.com/SlongLiu/DAB-DETR)]
* [(DN-DETR CVPR2022 OralPaper) DN-DETR: Accelerate DETR Training by Introducing Query DeNoising](https://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html) [[codes|official](https://github.com/IDEA-Research/DN-DETR)]

### Datasets

* **** [[]()][[]()]

* **** [[]()][[]()]

### Papers

* **UnionDet(ECCV2020)** UnionDet: Union-Level Detector Towards Real-Time Human-Object Interaction Detection [[paper link](https://link.springer.com/chapter/10.1007/978-3-030-58555-6_30)][[codes|official]()]

* **MSTR(CVPR2022)** MSTR: Multi-Scale Transformer for End-to-End Human-Object Interaction Detection [[paper link](https://openaccess.thecvf.com/content/CVPR2022/html/Kim_MSTR_Multi-Scale_Transformer_for_End-to-End_Human-Object_Interaction_Detection_CVPR_2022_paper.html)][[codes|official]()]

* **DisTrans(CVPR2022)** Human-Object Interaction Detection via Disentangled Transformer [[paper link](https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Human-Object_Interaction_Detection_via_Disentangled_Transformer_CVPR_2022_paper.html)][[codes|official]()]



**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êDeep Neural Networks

### Frameworks

* **PyTorch** [Home Page](https://pytorch.org/), [Offical Documentation](https://pytorch.org/docs/stable/index.html)
* **TensorFlow** [Home Page](https://tensorflow.google.cn/), [Offical Documentation](https://tensorflow.google.cn/versions/r1.15/api_docs/python/tf)

### Materials

* [Efficient Processing of Deep Neural Networks: A Tutorial and Survey](https://arxiv.org/pdf/1703.09039.pdf)
* [(CSDN blog) ‰∫îÂ§ßÁªèÂÖ∏Âç∑ÁßØÁ•ûÁªèÁΩëÁªú‰ªãÁªçÔºöLeNet / AlexNet / GoogLeNet / VGGNet/ ResNet](https://blog.csdn.net/fendouaini/article/details/79807830)
* [(cnblogs) Deep LearningÂõûÈ°æ#‰πãLeNet„ÄÅAlexNet„ÄÅGoogLeNet„ÄÅVGG„ÄÅResNet](https://www.cnblogs.com/52machinelearning/p/5821591.html)
* [(github) HRNet: HRNet-Applications-Collection](https://github.com/HRNet/HRNet-Applications-Collection)

### Papers



**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êImage Mosaic


### Materials

* [(zhihu) Âü∫‰∫éÂõæÂÉèÁöÑ‰∏âÁª¥Âª∫Ê®°‚Äî‚ÄîÁâπÂæÅÁÇπÊ£ÄÊµã‰∏éÂåπÈÖç](https://zhuanlan.zhihu.com/p/128937547)
* [(website) ÂõæÂÉèÊãºÊé•ÁÆóÊ≥ïÁöÑÁªºËø∞ - A survey on image mosaicing techniques](http://s1nh.org/post/A-survey-on-image-mosaicing-techniques/)
* [(cnblogs) OpenCVÊé¢Á¥¢‰πãË∑ØÔºà‰∫åÂçÅÂõõÔºâÂõæÂÉèÊãºÊé•ÂíåÂõæÂÉèËûçÂêàÊäÄÊúØ](https://www.cnblogs.com/skyfsm/p/7411961.html)
* [(zhihu - YaqiLYU) ÂõæÂÉèÊãºÊé•Áé∞Âú®ËøòÊúâÁ†îÁ©∂ÁöÑ‰ª∑ÂÄºÂêóÔºüÊúâÂì™‰∫õÂèØ‰ª•Á†îÁ©∂ÁöÑÁÇπÔºüÁé∞Âú®ÊäÄÊúØÂèëÂ±ïÂ¶Ç‰ΩïÔºü](https://www.zhihu.com/question/34535199/answer/135169187)
* [(zhihu - YaqiLYU) ÁõÆÂâçÊúÄÊàêÁÜüÁöÑÂÖ®ÊôØËßÜÈ¢ëÊãºÊé•ÊäÄÊúØÊòØÊÄéÊ†∑ÁöÑÔºü](https://www.zhihu.com/question/34573969/answer/136464893)
* [(opencv docs) Feature Detection and Description](https://docs.opencv.org/master/db/d27/tutorial_py_table_of_contents_feature2d.html)
* [(github) [Real-Time Image Stitching] CS205 Computing Foundations for Computational Science Final Project(C++)](https://github.com/ziqiguo/CS205-ImageStitching)
* [(github) [Image and Video Stitching] Conducts image stitching upon an input video to generate a panorama in 3D(Python)](https://github.com/WillBrennan/ImageStitching)
* [(github) Multiple Image stitching in Python](https://github.com/kushalvyas/Python-Multiple-Image-Stitching)


### Papers

* **NISwGSP(ECCV2016)** Natural Image Stitching with the Global Similarity Prior [[paper link](https://link.springer.com/chapter/10.1007%2F978-3-319-46454-1_12)][[Codes|offical C++ & Matlab](https://github.com/nothinglo/NISwGSP)]

* **VFSMS(CMS2019)** A Fast Algorithm for Material Image Sequential Stitching [[paper link](http://www.sciencedirect.com/science/article/pii/S0927025618307158)][[software](https://www.mgedata.cn/app_entrance/microscope)][[Codes|offical python & C++](https://github.com/Keep-Passion/ImageStitch)]

**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êImage Restoration

Image restoration includes **image in-painting**, **pixel interpolation**, **image deblurring**, and **image denoising**.
 
### Materials

* [(github) CNN-For-End-to-End-Deblurring (Keras)](https://github.com/axium/CNN-For-End-to-End-Deblurring--Keras)

### Papers

* **DnCNN(TIP2017)** Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising [[paper link](http://www4.comp.polyu.edu.hk/~cslzhang/paper/DnCNN.pdf)][[Codes|MATLAB(offical)](https://github.com/cszn/DnCNN)]

* **MemNet(ICCV2017)** MemNet: A Persistent Memory Network for Image Restoration [[paper link](http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf)][[Codes|Matlab(offical)](https://github.com/tyshiwo/MemNet)]

* **pix2pix(CVPR2017)** Image-to-Image Translation with Conditional Adversarial Nets [[arxiv link](https://arxiv.org/abs/1611.07004)][[project link](https://phillipi.github.io/pix2pix/)][[Codes|Torch(offical)](https://github.com/phillipi/pix2pix)][[Codes|PyTorch(offical)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)]

* **DeepDeblur(CVPR2017)** [[paper link](https://openaccess.thecvf.com/content_cvpr_2017/papers/Nah_Deep_Multi-Scale_Convolutional_CVPR_2017_paper.pdf)][[Codes|Torch(offical)](https://github.com/SeungjunNah/DeepDeblur_release)][[Codes|PyTorch(offical)](https://github.com/SeungjunNah/DeepDeblur-PyTorch)]

* **ImageDeblurring(ICCV2017)** Deep Generative Filter for motion deblurring [[arxiv link](https://arxiv.org/abs/1709.03481)][[Codes|Keras&Tensorflow(offical)](https://github.com/leftthomas/ImageDeblurring)]

* **DeblurGAN(CVPR2017)** DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks [[arxiv link](https://arxiv.org/pdf/1711.07064.pdf)][[Codes|PyTorch(offical)](https://github.com/KupynOrest/DeblurGAN)]

* **SRN-Deblur(CVPR2018)** Scale-recurrent Network for Deep Image Deblurring [[paper link](http://www.xtao.website/projects/srndeblur/srndeblur_cvpr18.pdf)][[Codes|Tensorflow(offical)](https://github.com/jiangsutx/SRN-Deblur)]

* **RNN-Deblur(CVPR2018)** Dynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks [[paper link](https://www.cs.cityu.edu.hk/~rynson/papers/cvpr18c.pdf)][[Codes|Matcaffe(offical)](https://github.com/zhjwustc/cvpr18_rnn_deblur_matcaffe)]

* **Deep-Semantic-Face(CVPR2018)** Deep Semantic Face Deblurring [[paper link](https://research.nvidia.com/sites/default/files/pubs/2018-06_Deep-Semantic-Face//DeepSemanticFaceDeblur_CVPR18.pdf)][[project link](https://research.nvidia.com/publication/2018-06_Deep-Semantic-Face)][[Codes|Matlab(offical)](https://github.com/joanshen0508/Deep-Semantic-Face-Deblurring)]

**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êLane Detection

### Materials

### Datasets

* [**CULane**: SCNN(AAAI2018) Spatial As Deep: Spatial CNN for Traffic Scene Understanding](https://xingangpan.github.io/projects/CULane.html)

### Papers
 
* **SCNN(AAAI2018)** Spatial As Deep: Spatial CNN for Traffic Scene Understanding [[arxiv link](https://arxiv.org/abs/1712.06080)][[Codes|offical Torch & Matlab](https://github.com/XingangPan/SCNN)]

* **‚ù§ LaneNet(IVS2018)** Towards End-to-End Lane Detection: an Instance Segmentation Approach [[arxiv link](https://arxiv.org/abs/1802.05591)][[project link](https://maybeshewill-cv.github.io/lanenet-lane-detection/)][[Codes|unoffical TF](https://github.com/MaybeShewill-CV/lanenet-lane-detection)]

* **‚ù§ UltraLane(ECCV2020)** Ultra Fast Structure-aware Deep Lane Detection [[arxiv link](https://arxiv.org/abs/2004.11757)][[Codes|offical PyTorch](https://github.com/cfzd/Ultra-Fast-Lane-Detection)]

* **B√©zierLaneNet(CVPR2022)** Rethinking Efficient Lane Detection via Curve Modeling [[paper link](https://arxiv.org/abs/2203.02431)][[codes|official PyTorch](https://github.com/voldemortX/pytorch-auto-drive)]



**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êPedestrian Localization

### Materials


### Papers

* **Monoloco(ICCV2019)** MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation [[arxiv link](https://arxiv.org/abs/1906.06059)][[Codes|PyTorch(offical)](https://github.com/vita-epfl/monoloco)]

**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êPerson ReID

### Materials

* [(zhihu) ‰ªéÈõ∂ÂºÄÂßãË°å‰∫∫ÈáçËØÜÂà´](https://zhuanlan.zhihu.com/p/50387521)
* [(zhihu) (ËΩ¨)Ë°å‰∫∫ÈáçËØÜÂà´(ReID) ‚Äî‚ÄîÊäÄÊúØÂÆûÁé∞ÂèäÂ∫îÁî®Âú∫ÊôØ](https://zhuanlan.zhihu.com/p/64362368)
* [(zhihu) ‰∏Ä‰∫õÊÉ≥Ê≥ïÔºöÂÖ≥‰∫éË°å‰∫∫Ê£ÄÊµã‰∏éÈáçËØÜÂà´](https://zhuanlan.zhihu.com/p/39282286)
* [(zhihu) Èõ∂Âü∫Á°ÄÂÆûÊàòË°å‰∫∫ÈáçËØÜÂà´ReIDÈ°πÁõÆ-Âü∫‰∫éMilvusÁöÑ‰ª•ÂõæÊêúÂõæ](https://zhuanlan.zhihu.com/p/141204192)
* [(csdnblog) Ë°å‰∫∫ÈáçËØÜÂà´ÔºàPerson Re-IDÔºâ„Äê‰∏Ä„ÄëÔºöÂ∏∏Áî®ËØÑÊµãÊåáÊ†á](https://blog.csdn.net/qq_38451119/article/details/83000061)
* [(csdnblog) ‰∫ë‰ªéÁßëÊäÄÔºöËØ¶Ëß£Ë∑®ÈïúËøΩË∏™ÔºàReIDÔºâÊäÄÊúØÂÆûÁé∞ÂèäÂ∫îÁî®Âú∫ÊôØ](https://edu.csdn.net/course/detail/8426)
* [(tencent cloud) ‰∫ë‰ªéÁßëÊäÄËµÑÊ∑±ÁÆóÊ≥ïÁ†îÁ©∂ÂëòÔºöËØ¶Ëß£Ë∑®ÈïúËøΩË∏™(ReID)ÊäÄÊúØÂÆûÁé∞ÂèäÈöæÁÇπ | ÂÖ¨ÂºÄËØæÁ¨îËÆ∞](https://cloud.tencent.com/developer/article/1160607)

### Datasets

* [Market1501 [Tsinghua University; 32217 images; 1501 persons; 6 cameras]](http://liangzheng.com.cn/Project/project_reid.html)
* [DukeMTMC-ReID [Duke University; 36441 images; 1812 persons; 8 cameras]](https://github.com/sxzrt/DukeMTMC-reID_evaluation#download-dataset)
* [CUHK03 [CUHK University; 13164 images; 1467 persons; 10 cameras]](http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html)

### Papers

* **(TOMM2017)** A Discriminatively Learned CNN Embedding for Person Re-identification [[arxiv link](https://arxiv.org/pdf/1611.05666.pdf)][[Codes|caffe+keras(official)](https://github.com/layumi/2016_person_re-ID)][[CSDN blog](https://blog.csdn.net/weixin_41427758/article/details/80091596)]



**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êScene Text Detection

### Materials

* [Commonly used four datasets in scene text detection: ICDAR2015, CTW1500, Total-Text and MSRA-TD500]
* [(github) üöÄPaddleOCR: Awesome multilingual OCR toolkits based on PaddlePaddle](https://github.com/PaddlePaddle/PaddleOCR)

### Papers

#### Regression-based

* **R2CNN(arxiv2017)** R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection [[paper link](https://arxiv.org/abs/1706.09579)]

* **TextBoxes(AAAI2017)** TextBoxes: A Fast Text Detector with a Single Deep Neural Network [[paper link](https://ojs.aaai.org/index.php/AAAI/article/view/11196)]

* **‚ù§ EAST(CVPR2017)** EAST: An Efficient and Accurate Scene Text Detector [[paper link](https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_EAST_An_Efficient_CVPR_2017_paper.html)]

* **ContourNet(CVPR2020)** ContourNet: Taking a Further Step Toward Accurate Arbitrary-Shaped Scene Text Detection [[paper link](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_ContourNet_Taking_a_Further_Step_Toward_Accurate_Arbitrary-Shaped_Scene_Text_CVPR_2020_paper.html)][[codes|official](https://github.com/wangyuxin87/ContourNet)]

* **ABCNet(CVPR2020)** ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network [[paper link](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_ABCNet_Real-Time_Scene_Text_Spotting_With_Adaptive_Bezier-Curve_Network_CVPR_2020_paper.pdf)][[codes|Detectron2 & AdelaiDet Toolbox](https://github.com/aim-uofa/AdelaiDet)]

* **ABCNet_v2(TPAMI2021)** ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting [[paper link](https://ieeexplore.ieee.org/abstract/document/9525302)][[codes|Detectron2 & AdelaiDet Toolbox](https://github.com/aim-uofa/AdelaiDet)]

#### Segmentation-based

* **TextSnake(ECCV2018)** TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes [[paper link](https://openaccess.thecvf.com/content_ECCV_2018/html/Shangbang_Long_TextSnake_A_Flexible_ECCV_2018_paper.html)]

* **TextDragon(ICCV2019)** TextDragon: An End-to-End Framework for Arbitrary Shaped Text Spotting [[paper link](https://openaccess.thecvf.com/content_ICCV_2019/html/Feng_TextDragon_An_End-to-End_Framework_for_Arbitrary_Shaped_Text_Spotting_ICCV_2019_paper.html)]

* **PANet(ICCV2019)** Efficient and Accurate Arbitrary-Shaped Text Detection With Pixel Aggregation Network [[paper link](https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.html)]

* **PSENet(CVPR2019)** Shape Robust Text Detection With Progressive Scale Expansion Network [[paper link](https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.html)]

* **DBNet(AAAI2020)** Real-Time Scene Text Detection with Differentiable Binarization [[paper link](https://ojs.aaai.org/index.php/AAAI/article/view/6812)][[codes|official](https://github.com/MhLiao/DB)]

* **FCENet(CVPR2021)** Fourier Contour Embedding for Arbitrary-Shaped Text Detection [paper link](https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Fourier_Contour_Embedding_for_Arbitrary-Shaped_Text_Detection_CVPR_2021_paper.html)]


**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**


## ‚≠êSemantic Segmentation

### Materials

* [(CSDN blogs) ËØ≠‰πâÂàÜÂâ≤ - Semantic Segmentation Papers](https://blog.csdn.net/langb2014/article/details/82414918)

### Papers

* **FCIS(CVPR2017)** Fully Convolutional Instance-aware Semantic Segmentation [[arxiv link](https://arxiv.org/abs/1611.07709)][[Codes|MXNet(offical based on RFCN)](https://github.com/msracver/FCIS)][[CSDN blog](https://blog.csdn.net/jiongnima/article/details/78961147)]

* **BezierSeg(arxiv2021)** BezierSeg: Parametric Shape Representation for Fast Object Segmentation in Medical Images [[paper link](https://arxiv.org/abs/2108.00760)]

**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êSound Source Localization

### Materials

* [(cnblogs) „ÄêËÆ∫ÊñáÂØºËØª„ÄëLearning to Localize Sound Source in Visual Scenes„Äë&soundnetÁöÑÂ§çÁé∞](https://blog.csdn.net/zzc15806/article/details/80772152)
* [(cnblogs) ËÆ∫Êñá„ÄêLearning to Localize Sound Source in Visual Scenes„Äë&soundnetÁöÑÂ§çÁé∞](https://www.cnblogs.com/gaoxiang12/p/3695962.html)
* [(CSDNblogs) È∫¶ÂÖãÈ£éÈòµÂàóÂ£∞Ê∫êÂÆö‰Ωç GCC-PHAT](https://blog.csdn.net/u010592995/article/details/79735198)
* [(online PPT) ËØ≠Èü≥ËØÜÂà´ÊäÄÊúØÁöÑÂâç‰∏ñ‰ªäÁîü(made by ÁéãËµü(Maigo))](https://zhihu-live.zhimg.com/0af15bfda98f5885ffb509acd470b0fa)


### Datasets

* **Columbia dataset (ECCV2016)** Cross-modal Supervision for Learning Active Speaker Detection in Video [[paper link](https://link.springer.com/chapter/10.1007/978-3-319-46454-1_18)]

* **LRS2 (TPAMI2018)** Deep Audio-Visual Speech Recognition [[paper link](https://ieeexplore.ieee.org/abstract/document/8585066)][[dataset link](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/)]

* **LRS3 (arxiv2018)** LRS3-TED: a large-scale dataset for visual speech recognition [[paper link](https://arxiv.org/abs/1809.00496)][[dataset link](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/)]

* **dataset annotation tool: VIA (ACMMM2019)** The VIA Annotation Software for Images, Audio and Video [[paper link](https://www.robots.ox.ac.uk/~adutta/data/postdoc/dutta2019vgg.pdf)][[project link](https://www.robots.ox.ac.uk/~vgg/software/via/)]


### Papers

* **‚ù§ SoundNet(NIPS2016)** SoundNet: Learning Sound Representations from Unlabeled Video [[arxiv link](https://arxiv.org/pdf/1610.09001.pdf)][[project link](http://projects.csail.mit.edu/soundnet/)][[Codes|offical TensorFlow](https://github.com/cvondrick/soundnet)][[CSDN blog](https://blog.csdn.net/zzc15806/article/details/80669883)](`Dataset: SoundNet`)

* **AVC(ICCV2017)** Look, Listen and Learn [[paper link](https://openaccess.thecvf.com/content_iccv_2017/html/Arandjelovic_Look_Listen_and_ICCV_2017_paper.html)]

* **Multisensory(ECCV2018)** Audio-Visual Scene Analysis with Self-Supervised Multisensory Features [[paper link](https://openaccess.thecvf.com/content_ECCV_2018/html/Andrew_Owens_Audio-Visual_Scene_Analysis_ECCV_2018_paper.html)][[project link](https://andrewowens.com/multisensory/)][[codes|officical TensorFlow](https://github.com/andrewowens/multisensory)]

* **‚ù§ SoundLocation or Attention10k(CVPR2018)** Learning to Localize Sound Source in Visual Scenes [[arxiv link](https://arxiv.org/pdf/1803.03849.pdf)][[Codes|offical PyTorch based on SoundNet](https://github.com/ardasnck/learning_to_localize_sound_source)][[Codes|unofficial PyTorch](https://github.com/liyidi/soundnet_localize_sound_source)](`Dataset: Flickr-SoundNet`)

* **DMC(CVPR2019)** Deep Multimodal Clustering for Unsupervised Audiovisual Learning [[paper link](https://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Deep_Multimodal_Clustering_for_Unsupervised_Audiovisual_Learning_CVPR_2019_paper.html)]

* **MSSL(ECCV2020)** Multiple Sound Sources Localization from Coarse to Fine [[paper link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650290.pdf)][[codes|official PyTorch](https://github.com/shvdiwnkozbw/Multi-Source-Sound-Localization)][[Author - [Weiyao Lin]](https://weiyaolin.github.io/)]

* **AVVP(ECCV2020)** Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing [[paper link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480443.pdf)][[codes|official PyTorch](https://github.com/YapengTian/AVVP-ECCV20)]

* **AVObjects(ECCV2020)** Self-Supervised Learning of Audio-Visual Objects from Video [[paper link](https://arxiv.org/abs/2008.04237)][[project link](https://www.robots.ox.ac.uk/~vgg/research/avobjects/)][[Oxford VGG](https://www.robots.ox.ac.uk/~vgg/)][[codes|official PyTorch](https://github.com/afourast/avobjects)]

* **‚ù§ LVS or VGG-Sound(CVPR2021)** Localizing Visual Sounds the Hard Way [[paper link](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Localizing_Visual_Sounds_the_Hard_Way_CVPR_2021_paper.html)][[codes|official PyTorch](https://github.com/hche11/Localizing-Visual-Sounds-the-Hard-Way)](`Dataset: VGG-Sound Source`)

* **vanilla-LVS or HardPos(ICASSP2022)** Learning Sound Localization Better from Semantically Similar Samples [[paper link](https://ieeexplore.ieee.org/abstract/document/9747867)]

* **‚ù§ EZ-VSL(arxiv2022)** Localizing Visual Sounds the Easy Way [[paper link](https://arxiv.org/abs/2203.09324)][[codes|official PyTorch](https://github.com/stoneMo/EZ-VSL)][[multiple-instance-learning](https://proceedings.mlr.press/v80/ilse18a/ilse18a.pdf)]

* **IEr(AAAI2022)** Visual Sound Localization in the Wild by Cross-Modal Interference Erasing [[paper link](https://www.aaai.org/AAAI22Papers/AAAI-140.LiuX.pdf)][[codes|official](https://github.com/alvinliu0/Visual-Sound-Localization-in-the-Wild)]

**-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-**

## ‚≠êTraffic Violation Detection

### Materials

* [(github) Traffic-Rule-Violation-Detection-System (Tensorflow + OpenALPR )](https://github.com/ShreyAmbesh/Traffic-Rule-Violation-Detection-System)
* [(github) Traffic-Signal-Violation-Detection-System (Tensorflow based YOLOv3)](https://github.com/anmspro/Traffic-Signal-Violation-Detection-System)
* [(github) Traffic-Rules-Violation-Detection (mobilenet-v1)](https://github.com/rahatzamancse/Traffic-Rules-Violation-Detection)
* [(github) Traffic-Rules-Violation-Detection-System (mobilenet-v1)](https://github.com/sakibreza/Traffic-Rules-Violation-Detection-System)
* [(github) Fully-Automated-red-light-Violation-Detection (Tensorflow based YOLOv3)](https://github.com/AhmadYahya97/Fully-Automated-red-light-Violation-Detection)
* [(github) yolov3-vehicle-detection-paddle](https://github.com/Sharpiless/yolov3-vehicle-detection-paddle) [[CSDN link](https://blog.csdn.net/weixin_45449540/article/details/107345738)]

### Papers



