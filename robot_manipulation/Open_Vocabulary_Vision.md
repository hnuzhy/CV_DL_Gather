# ‚≠êOpen Vocabulary Vision
*also known as `Open World/Vocabulary Detection`, `Open World/Vocabulary Segmentation`*

## Materials

## Datasets

## Papers

### 1) Open World/Vocabulary Detection

* üëç**Detic(ECCV2022)(arxiv2022.01)** Detecting Twenty-thousand Classes using Image-level Supervision [[paper link](https://link.springer.com/chapter/10.1007/978-3-031-20077-9_21)][[arxiv link](https://arxiv.org/abs/2201.02605)][[code|official](https://github.com/facebookresearch/Detic)][`Meta AI` and `The University of Texas at Austin`]

* **Openset-RCNN(RAL2023)(arxiv2022.11)** Open-Set Object Detection Using Classification-Free Object Proposal and Instance-Level Contrastive Learning [[paper link](https://ieeexplore.ieee.org/abstract/document/10035923)][[arxiv link](https://arxiv.org/abs/2211.11530)][[project link](https://ieeexplore.ieee.org/abstract/document/10035923)][[code|official](https://github.com/Yifei-Y/Openset-RCNN)][`ZJU`]

* üëç**YOLO-World(CVPR2024)(arxiv2024.01)** YOLO-World: Real-Time Open-Vocabulary Object Detection [[paper link](https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_YOLO-World_Real-Time_Open-Vocabulary_Object_Detection_CVPR_2024_paper.html)][[arxiv link](https://arxiv.org/abs/2401.17270)][[project link](https://www.yoloworld.cc/)][[code|official](https://github.com/AILab-CVC/YOLO-World)][[huggingface link](https://huggingface.co/spaces/stevengrove/YOLO-World)][`Tencent AI Lab + ARC Lab, Tencent PCG + Huazhong University of Science and Technology`]

* üëç**YOLO-UniOW(arxiv2024.12)** YOLO-UniOW: Efficient Universal Open-World Object Detection [[arxiv link](https://arxiv.org/abs/2412.20645)][[code|official](https://github.com/THU-MIG/YOLO-UniOW)][`Tsinghua University + Tencent ARC Lab`]


### 2) Open World/Vocabulary Segmentation

* **VLPart(ICCV2023)(arxiv2023.05)** Going Denser with Open-Vocabulary Part Segmentation [[paper link](https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Going_Denser_with_Open-Vocabulary_Part_Segmentation_ICCV_2023_paper.html)][[arxiv link](https://arxiv.org/abs/2305.11173)][[code|official](https://github.com/facebookresearch/VLPart)][`he University of Hong Kong + Meta AI + New York University + Shanghai AI Laboratory`]

* **SATR(ICCV2023 oral)(arxiv2023.04)** SATR: Zero-Shot Semantic Segmentation of 3D Shapes [[paper link](https://openaccess.thecvf.com/content/ICCV2023/html/Abdelreheem_SATR_Zero-Shot_Semantic_Segmentation_of_3D_Shapes_ICCV_2023_paper.html)][[arxiv link](https://arxiv.org/abs/2304.04909)][[project link](https://samir55.github.io/SATR/)][[code|official](https://github.com/Samir55/SATR)][`KAUST + LIX, Ecole Polytechnique`]

* üëç**PartSLIP(CVPR2023)(arxiv2022.12)** PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models [[paper link](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_PartSLIP_Low-Shot_Part_Segmentation_for_3D_Point_Clouds_via_Pretrained_CVPR_2023_paper.html)][[arxiv link](http://arxiv.org/abs/2212.01558)][[project link](https://colin97.github.io/PartSLIP_page/)][[code|official](https://drive.google.com/drive/u/0/folders/19j6PZfW8TDQ1ifHZwHIhn6X4BHjYRFCL)][`University of California San Diego + Qualcomm AI Research`; `Hao Su`]

* **OV-PartS(NIPS2023)(arxiv2023.10)** OV-PARTS: Towards Open-Vocabulary Part Segmentation [[paper link](https://proceedings.neurips.cc/paper_files/paper/2023/hash/dde53059fdb0f45e1e9ad9c66997d662-Abstract-Datasets_and_Benchmarks.html)][[arxiv link](https://arxiv.org/abs/2310.05107)][[code|official](https://github.com/OpenRobotLab/OV_PARTS)][`Shanghai AI Laboratory + The University of Hong Kong + The University of Sydney + University of Macau + Texas A&M University`]

* üëç**SegTTO(arxiv2025.01)** Test-Time Optimization for Domain Adaptive Open Vocabulary Segmentation [[arxiv link](https://arxiv.org/abs/2501.04696)][[code|official](https://github.com/UlinduP/SegTTO)][`University of Moratuwa + Stony Brook University + Khalifa University`]

### 3) Zero-Shot Scene Flow Estimation

* **ZeroMSF(arxiv2025.01)** Zero-Shot Monocular Scene Flow Estimation in the Wild [[arxiv link](https://arxiv.org/abs/2501.10357)][[project link](https://research.nvidia.com/labs/lpr/zero_msf//)][`NVIDIA Research + Brown University`]

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]


