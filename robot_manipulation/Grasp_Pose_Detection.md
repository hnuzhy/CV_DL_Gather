# Grasp Pose Detection
*also belonging to `Manipulation Strategy` and `Grasp Prediction/Generation`*

## Materials

* [**paperswithcode** The Ranking of Robotic Grasping on GraspNet-1Billion](https://paperswithcode.com/sota/robotic-grasping-on-graspnet-1billion)

***

## Datasets

* üëç**GraspNet-1Billion(CVPR2020)** GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping [[paper link](https://openaccess.thecvf.com/content_CVPR_2020/html/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.html)][[project link](www.graspnet.net)][[code|official](https://github.com/graspnet/graspnet-baseline)][[paperswithcode link](https://paperswithcode.com/dataset/graspnet-1billion)][`SJTU`]

* üëç**ACRONYM(ICRA2021)(arxiv2020.11)** ACRONYM: A Large-Scale Grasp Dataset Based on Simulation [[paper link](https://ieeexplore.ieee.org/abstract/document/9560844/)][[arxiv link](https://arxiv.org/abs/2011.09584)][[project link](https://sites.google.com/nvidia.com/graspdataset)][[code|official](https://github.com/NVlabs/acronym)][`NVIDIA + University of Washington`]
 
* **SuctionNet-1Billion(RAL2021)(arxiv2021.03)** SuctionNet-1Billion: A Large-Scale Benchmark for Suction Grasping [[paper link](https://ieeexplore.ieee.org/abstract/document/9547830/)][[arxiv link](https://arxiv.org/abs/2103.12311)][[project link](https://graspnet.net/suction)][[code|official](https://github.com/graspnet/suctionnet-baseline)][`SJTU`]
   
* **REGRAD(RAL2022)(arxiv2021.04)** REGRAD: A Large-Scale Relational Grasp Dataset for Safe and Object-Specific Robotic Grasping in Clutter
  [[paper link](https://ieeexplore.ieee.org/abstract/document/9681218/)][[arxiv link](https://arxiv.org/abs/2104.14118)][[dataset link](https://stuxjtueducn-my.sharepoint.com/personal/chaser123_stu_xjtu_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fchaser123%5Fstu%5Fxjtu%5Fedu%5Fcn%2FDocuments%2FREGRAD%5Fv1&ga=1)][[code|official](https://github.com/poisonwine/REGRAD)][`XJTU`; `RElational GRAsps Dataset (REGRAD)`]

* **TransCG(RAL2022)(arxiv2022.02)** TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and a Grasping Baseline [[paper link](https://ieeexplore.ieee.org/abstract/document/9796631)][[arxiv link](https://arxiv.org/abs/2202.08471)][[project link](http://www.graspnet.net/transcg)][[code|official](https://github.com/galaxies99/TransCG)][`SJTU`]

* **DA2-Dataset(RAL2022)(arxiv2022.08)** DA2 Dataset: Toward Dexterity-Aware Dual-Arm Grasping [[paper link](https://ieeexplore.ieee.org/abstract/document/9826816/)][[arxiv link](https://arxiv.org/abs/2208.00408)][[project link](https://sites.google.com/view/da2dataset)][[dataset link](https://sites.google.com/view/da2dataset/dataset)][[code|official](https://sites.google.com/view/da2dataset/code)][`Technical University of Munich +  Tencent Robotics X + Imperial College London + ZJU + JHU`]
 
* üëç**OCRTOC(RAL2022)(arxiv2021.04)** OCRTOC: A Cloud-Based Competition and Benchmark for Robotic Grasping and Manipulation [[paper link](https://ieeexplore.ieee.org/abstract/document/9619915/)][[arxiv link](https://arxiv.org/abs/2104.11446)][[project link](https://www.ocrtoc.org/)][[code|official](https://github.com/OCRTOC/OCRTOC_software_package)][`Alibaba AI Labs + UC San Diego + University of Edinburgh + German Aerospace Center`]
   
* **ClothObjectSet(RAL2022)(arxiv2021.11)** Household Cloth Object Set: Fostering Benchmarking in Deformable Object Manipulation [[paper link](https://ieeexplore.ieee.org/abstract/document/9732698/)][[arxiv link](https://arxiv.org/abs/2111.01527)][[project link](https://www.iri.upc.edu/groups/perception/ClothObjectSet/)][`Spain`]

* üëç**DexGraspNet(ICRA2023)(arxiv2022.10)** DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation [[paper link](https://ieeexplore.ieee.org/abstract/document/10160982/)][[arxiv link](https://arxiv.org/abs/2210.02697)][[project link](https://pku-epic.github.io/DexGraspNet/)][[code|official](https://github.com/PKU-EPIC/DexGraspNet)][`PKU`]

* **MetaGraspNetV2(TASE2023)** MetaGraspNetV2: All-in-One Dataset Enabling Fast and Reliable Robotic Bin Picking via Object Relationship Reasoning and Dexterous Grasping [[paper link](https://ieeexplore.ieee.org/abstract/document/10309974/)][[code|official](https://github.com/maximiliangilles/MetaGraspNet)][`Karlsruher Institut f√ºr Technologie (KIT)`]
 
* üëç**Sim-Suction(TRO2023)(arxiv2023.05)** Sim-Suction: Learning a Suction Grasp Policy for Cluttered Environments Using a Synthetic Benchmark [[paper link](https://ieeexplore.ieee.org/abstract/document/10314015)][[arxiv link](https://arxiv.org/abs/2305.16378)][[project link](https://junchengli1.github.io/Sim-Suction/)][[code|official](https://github.com/junchengli1/Sim-Suction-API)][`Purdue University`]

* **CoAS-Net(RAL2024)** CoAS-Net: Context-Aware Suction Network With a Large-Scale Domain Randomized Synthetic Dataset [[paper link](https://ieeexplore.ieee.org/abstract/document/10333337)][[code|official](https://github.com/SonYeongGwang/CoAS-Net.git)][`Sungkyunkwan University`]

* üëç**Grasp-Anything(ICRA2024)(arxiv2023.09)** Grasp-Anything: Large-scale Grasp Dataset from Foundation Models [[arxiv link](https://arxiv.org/abs/2309.09818)][[project link](https://grasp-anything-2023.github.io/)][`FPT Software - AIC Lab (Hanoi, Vietnam)`]

***

## Papers

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]

* üëç**graspnet-baseline(CVPR2020)** GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping [[paper link](https://openaccess.thecvf.com/content_CVPR_2020/html/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.html)][[project link](www.graspnet.net)][[code|official](https://github.com/graspnet/graspnet-baseline)][[paperswithcode link](https://paperswithcode.com/dataset/graspnet-1billion)][`SJTU`]

* **Graspness(ICCV2021)** Graspness Discovery in Clutters for Fast and Accurate Grasp Detection [[paper link](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Graspness_Discovery_in_Clutters_for_Fast_and_Accurate_Grasp_Detection_ICCV_2021_paper.html)][[graspnetAPI link](https://github.com/graspnet/graspnetAPI)][[project link](https://graspnet.net/)][[code|official](https://github.com/rhett-chen/graspness_implementation)][`SJTU`]

* **Scale-Balanced-Grasp(CoRL2022)(arxiv2022.12)** Towards Scale Balanced 6-DoF Grasp Detection in Cluttered Scenes [[paper link](https://proceedings.mlr.press/v205/ma23a.html)][[arxiv link](https://arxiv.org/abs/2212.05275)][[project link]()][[code|official](https://github.com/mahaoxiang822/Scale-Balanced-Grasp)][`Beihang University`]

* üëç**HGGD(RAL2023)(arxiv2024.03)** [[paper link](https://ieeexplore.ieee.org/document/10168242)][[arxiv link](https://arxiv.org/pdf/2403.18546)][[bilibili link](https://www.bilibili.com/video/BV1hH4y1H7qv/)][[code|official](https://github.com/THU-VCLab/HGGD)][`THU`][`Attention`: HGGD detects grasps only from `heatmap guidance`, without `any workspace mask` (adopted in [`Graspness`](https://github.com/rhett-chen/graspness_implementation)) or `object/foreground segmentation` method (adopted in [`Scale-balanced Grasp`](https://github.com/mahaoxiang822/scale-balanced-grasp)). It may be useful to add some of this prior information to get better results.]

* **** [[paper link]()][[arxiv link]()][[project link]()][[code|official]()]
