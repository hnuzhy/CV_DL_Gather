# ‚≠êVision Language Pretraining
*also known as `Vision Language Models` and `Large Multimodal Models`*

## Materials

## Datasets

## Papers

* **Flamingo(NIPS2022)(arxiv2022.04)** Flamingo: a Visual Language Model for Few-Shot Learning [[paper link](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)][[arxiv link](https://arxiv.org/abs/2204.14198)][[blog|official](https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/)][`DeepMind`, Flamingo is a family of `Visual Language Models (VLM)`]

* **OpenFlamingo(arxiv2023.08)** OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models [[arxiv link](https://arxiv.org/abs/2308.01390)][[code|official](https://github.com/mlfoundations/open_flamingo)][[blog 1 | official](https://laion.ai/blog/open-flamingo/)][[blog 2 | official](https://laion.ai/blog/open-flamingo-v2/)][`University of Washington` and `Stanford University`]

* **TiC-CLIP(ICLR2024)(arxiv2023.10)** TiC-CLIP: Continual Training of CLIP Models [[openreview link](https://openreview.net/forum?id=TLADT8Wrhn)][[arxiv link](https://arxiv.org/abs/2310.16226)][[code|official](https://github.com/apple/ml-tic-clip)][`Apple`, based on the code of [`OpenCLIP`](https://github.com/mlfoundations/open_clip)]


