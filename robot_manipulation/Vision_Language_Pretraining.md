# ‚≠êVision Language Pretraining
*also known as `Vision Language Models`, `Vision Language Representation Learning` and `Large Multimodal Models`*

***

## Materials

## Datasets

## Papers

### ‚ñ∂ Vision Language Pretraining

* **Flamingo(NIPS2022)(arxiv2022.04)** Flamingo: a Visual Language Model for Few-Shot Learning [[paper link](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)][[arxiv link](https://arxiv.org/abs/2204.14198)][[blog|official](https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/)][`DeepMind`, Flamingo is a family of `Visual Language Models (VLM)`]

* **OpenFlamingo(arxiv2023.08)** OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models [[arxiv link](https://arxiv.org/abs/2308.01390)][[code|official](https://github.com/mlfoundations/open_flamingo)][[blog 1 | official](https://laion.ai/blog/open-flamingo/)][[blog 2 | official](https://laion.ai/blog/open-flamingo-v2/)][`University of Washington` and `Stanford University`]

* **TiC-CLIP(ICLR2024)(arxiv2023.10)** TiC-CLIP: Continual Training of CLIP Models [[openreview link](https://openreview.net/forum?id=TLADT8Wrhn)][[arxiv link](https://arxiv.org/abs/2310.16226)][[code|official](https://github.com/apple/ml-tic-clip)][`Apple`, based on the code of [`OpenCLIP`](https://github.com/mlfoundations/open_clip)]

* **Cluster Masking(CVPR2024)(arxiv2024.05)** Efficient Vision-Language Pre-training by Cluster Masking [[paper link]()][[arxiv link](https://arxiv.org/abs/2405.08815)][[project link](https://zxp46.github.io/cluster-masking/)][[code|official](https://github.com/Zi-hao-Wei/Efficient-Vision-Language-Pre-training-by-Cluster-Masking)][`University of Michigan`]

* üëç**Florence-2(CVPR2024)(arxiv2023.11)** Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks [[paper link](https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.html)][[arxiv link](https://arxiv.org/abs/2311.06242)][[project link](https://blog.roboflow.com/florence-2/)][[code|official](https://github.com/kijai/ComfyUI-Florence2)][[code|huggingface](https://huggingface.co/microsoft/Florence-2-large)][`Azure AI, Microsoft`]

* **ImOV3D(NIPS2024)(arxiv2024.10)** ImOV3D: Learning Open-Vocabulary Point Clouds 3D Object Detection from Only 2D Images [[openreview link](https://openreview.net/forum?id=RCO9fRP8AJ)][[arxiv link](https://arxiv.org/abs/2410.24001)][[code|official](https://github.com/yangtiming/ImOV3D)][`Shanghai Qi Zhi Institute + IIIS, Tsinghua University + Shanghai AI Lab`]


### ‚ñ∂ Vision Language Application

* **LQMFormer(CVPR2024)** LQMFormer: Language-aware Query Mask Transformer for Referring Image Segmentation [[paper link](https://openaccess.thecvf.com/content/CVPR2024/html/Shah_LQMFormer_Language-aware_Query_Mask_Transformer_for_Referring_Image_Segmentation_CVPR_2024_paper.html)][[code|(not available)]()][`Johns Hopkins University`; `Referring Image Segmentation (RIS)` aims to segment objects from an image based on a language description.]

* **RefHuman(NIPS2024)(arxiv2024.10)** Referring Human Pose and Mask Estimation in the Wild [[openreview link](https://openreview.net/forum?id=fXEi3LVflp)][[arxiv link](https://arxiv.org/abs/2410.20508)][[code|official](https://github.com/bo-miao/RefHuman)][`University of Western Australia + Xidian University + Hunan University + Griffith University`]


