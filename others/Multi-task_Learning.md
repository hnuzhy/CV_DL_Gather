# Multi-task Learning
> `Multi-loss Function`, or `Multi-task Optimization`, or `Mixture-of-Experts`, or `Multi-modal Task`


## Materials

* [**(github)** A collection of AWESOME things about mixture-of-experts (MoE)](https://github.com/XueFuzhao/awesome-mixture-of-experts)

## Papers

* üëç**MoE(Mixture-of-Experts)(ICLR2017)** Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer [[paper link](https://arxiv.org/abs/1701.06538)][[openreview link](https://openreview.net/forum?id=B1ckMDqlg)][`It allows the network to ensemble the outputs from different sub-networks without introducing too much extra computational costs`]

* **(ICLR2017)** Deep Multi-task Representation Learning: A Tensor Factorisation Approach [[paper link](https://arxiv.org/abs/1605.06391)][[openreview link](https://openreview.net/forum?id=SkhU2fcll)]

* üëç**Piggyback(ECCV2018)** Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights [[paper link](https://openaccess.thecvf.com/content_ECCV_2018/html/Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper.html)]

* **MARL(Multi-agent Reinforcement Learning)(ICLR2018)** Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning [[paper link](https://arxiv.org/abs/1711.01239)][[openreview link](https://openreview.net/forum?id=ry8dvM-R-)]

* **SFG(Stochastic Filter Groups)(ICCV2019)** Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels [[paper link](https://openaccess.thecvf.com/content_ICCV_2019/html/Bragman_Stochastic_Filter_Groups_for_Multi-Task_CNNs_Learning_Specialist_and_Generalist_ICCV_2019_paper.html)][`age regression + gender classification`, `semantic image regression (synthesis) + segmentation`]

* **(ICLR2020 rejected)** Feature Partitioning for Efficient Multi-Task Architectures [[paper link](https://arxiv.org/abs/1908.04339)][[openreview link](https://openreview.net/forum?id=B1eoyAVFwH)][`object recognition` or `image classification` task]

* üëç**AdaShare(NIPS2020)** AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning [[paper link](https://proceedings.neurips.cc/paper/2020/hash/634841a6831464b64c072c8510c7f35c-Abstract.html)][[project link](https://cs-people.bu.edu/sunxm/AdaShare/project.html)][[code|official](https://github.com/sunxm2357/AdaShare)]

* **TAPS(Task Adaptive Parameter Sharing)(CVPR2022)** Task Adaptive Parameter Sharing for Multi-Task Learning [[paper link](https://openaccess.thecvf.com/content/CVPR2022/html/Wallingford_Task_Adaptive_Parameter_Sharing_for_Multi-Task_Learning_CVPR_2022_paper.html)][[code|official](https://github.com/MattWallingford/TAPS)][`object recognition` or `image classification` task]

* **Uni-Perceiver(CVPR2022)** Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks [[paper link](http://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html)][[arxiv link](https://arxiv.org/abs/2112.01522)][[code|official](https://github.com/fundamentalvision/Uni-Perceiver)][`ViT Pre-training`, `It processes a variety of modalities and tasks with unified modeling and shared parameters`]

* **Uni-Perceiver-MoE(NIPS2022)** Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs [[paper link](https://arxiv.org/abs/2206.04674)][[openreview link](https://openreview.net/forum?id=agJEk7FhvKL)][[code|official](https://github.com/fundamentalvision/Uni-Perceiver)][`It proposes Conditional MoEs to address the task-interference issue in generalist models for the multi-task optimization`, `It is based on the generalist model Uni-Perceiver`]


