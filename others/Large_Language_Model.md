# Large Language Model (LLM) or Large Model

## Materials

* [**(github)** Luotuo-Chinese-LLM: È™ÜÈ©º(Luotuo): Open Sourced Chinese Language Models](https://github.com/LC1332/Luotuo-Chinese-LLM)
* [**(zhihu)** NLPÔºà‰πùÔºâÔºöLLaMA, Alpaca, ColossalChat Á≥ªÂàóÊ®°ÂûãÁ†îÁ©∂](https://zhuanlan.zhihu.com/p/618695885)
* [**(foundation work)** (Transformers)(NIPS2017) Attention is All you Need](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)

## Papers

### ‚ñ∂ NLP (Neural Language Processing)

* **GPT(generative pre-training)(2018)** Improving Language Understanding by Generative Pre-Training [[paper link](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)][`An autoregressive language model`, `pre-training model in NLP`, `OpenAI`]

* **GPT-2(2019)** Language Models are Unsupervised Multitask Learners [[[paper link](https://cs.brown.edu/courses/csci1460/assets/papers/language_models_are_unsupervised_multitask_learners.pdf)][`An autoregressive language model`, `pre-training model in NLP`, `OpenAI`]

* üëç**BERT(NAACL2019)** BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[paper link](https://arxiv.org/abs/1810.04805)][`An autoregressive language model`, `pre-training model in NLP`, `It uses masked language modeling (MLM) and next sentence prediction (NSP) for pre-training`]

* **GPT-3(NIPS2020)** Language Models are Few-Shot Learners [[paper link](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)][`An autoregressive language model`, `pre-training model in NLP`]


### ‚ñ∂ CV (Computer Vision)

* **iGPT(ICML2020)** Generative Pretraining From Pixels [[paper link](http://proceedings.mlr.press/v119/chen20s.html)][`It operates on sequences of pixels and predicts unknown pixels`]

* üëç**ViT(Vision Transformer)(ICLR2021)** An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale [[paper link](https://arxiv.org/abs/2010.11929)][[code|official](https://github.com/google-research/vision_transformer)][`Google`, `It studies masked patch prediction for self-supervised learning`][`ViT-Base / Large / Huge`]

* **ViTAE(NIPS2021)** ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias [[paper link](https://proceedings.neurips.cc/paper/2021/hash/efb76cff97aaf057654ef2f38cd77d73-Abstract.html)][[code|official](https://github.com/Annbless/ViTAE)][`ViT-based`, `self-supervised pre-training`, `Tao Dacheng`]

* **DINO(ICCV2021)** Emerging Properties in Self-Supervised Vision Transformers [[paper link](https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)][`ViT-based`, `a form of self-distillation with no labels`, `self-supervised pre-training`]

* **MoCo-v3(ICCV2021)** An Empirical Study of Training Self-Supervised Vision Transformers [[paper link](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)][`ViT-based`, `self-supervised pre-training`]

* **BEiT(arxiv2021.06)(ICLR2022 Oral)** BEiT: BERT Pre-Training of Image Transformers [[paper link](https://arxiv.org/abs/2106.08254)][[code|official](https://github.com/microsoft/unilm/tree/master/beit)][`ViT-based`, `self-supervised pre-training`, `MicroSoft`, `It proposes to predict discrete tokens`]

* üëç**MAE(CVPR2022)** Masked Autoencoders Are Scalable Vision Learners [[paper link](https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html)][`ViT-based`, `FAIR`, `He Kaiming`Ôºå `It reconstructs the original signal given its partial observation`, `self-supervised pre-training`]

* **ViTAEv2(IJCV2023)** ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond [[paper link](https://link.springer.com/article/10.1007/s11263-022-01739-w)][[arxiv link](https://arxiv.org/abs/2202.10108)][`ViT-based`, `self-supervised pre-training`, `Tao Dacheng`]


## ‚ñ∂ Super Stars

### ‚≠êSegment Anything

* üëç**SAM(arxiv2023.04)** Segment Anything [[arxiv link](https://arxiv.org/abs/2304.02643)][[project homepage](https://segment-anything.com/)][[publication link](https://ai.facebook.com/research/publications/segment-anything/)][[blogs](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)][[code|official](https://github.com/facebookresearch/segment-anything)][`Meta AI`, `Facebook`]

* **SSA(2023.04)** Semantic Segment Anything [[demo link](https://replicate.com/cjwbw/semantic-segment-anything)][[code|official](https://github.com/fudan-zvg/Semantic-Segment-Anything)][`Fudan`]

* **Grounded-SAM(2023.04)** Grounded Segment Anything [[code|official](https://github.com/IDEA-Research/Grounded-Segment-Anything)][`IDEA-Research`]
